---
title: "p8105_hw3_ak4123"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```
#Problem 1
##Data Cleaning: In this code chunk, I clean the data and focus on the "Overall Health Topic". Also, I include only responses from “Excellent” to “Poor”; also, I organize responses as a factor taking levels ordered from “Excellent” to “Poor”.
```{r question 1 data cleaning }
library(p8105.datasets)
data("brfss_smart2010")

brfss_data = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  filter(response == "Excellent" | response == "Very good" | response == "Good" | response == "Fair" | response == "Poor") %>% mutate(response = ordered(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor")))
```
##Part A:
```{r question 1 questions}
brfss_data %>%
  filter(year == 2002) %>% 
  distinct(locationdesc, locationabbr) %>%
  group_by(locationabbr) %>% 
  count() %>% 
  filter(n == 7)
```
The states with that were observed at 7 locations in 2002 are: CT, FL, and NC. 

##Part B: Here, I make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.
```{R}
brfss_data %>% 
  filter(year >= 2002 & year <= 2010) %>% 
  group_by(locationabbr, year) %>%  
  distinct(locationdesc) %>% 
  summarize(location_count = n()) %>%
  ggplot(aes(x = year, y = location_count, color = locationabbr)) +
    geom_line(size = 0.1) +
    labs(
      title = "Observation Plot",
      x = "Year",
      y = "Number of Observations",
      caption = "BRFSS Data"
    ) +
    viridis::scale_color_viridis(
      name = "State",
      discrete = TRUE
    ) + 
  theme_bw() +
    theme(legend.position = "bottom") +
  guides(color = guide_legend(nrow = 6))
```
As we can see from this line plot, most states stay fairly the same in terms of the number of observations, but there are a few that have some peaks. Because there are so many states, it is hard to really distinguish the colors from one another, but there is one state that appears to have a large increase in 2007 and again in 2010. Most of the states remain under 10 observations. 

##Part C: Here, I make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.
```{R}
brfss_data %>%
  spread(key = response, value = data_value) %>%
  janitor::clean_names() %>% 
  select(year, excellent, locationabbr) %>% 
  filter(!is.na(excellent) & locationabbr == "NY" & (year == "2002" | year == "2006" | year == "2010")) %>%
  mutate(prop_excellent = excellent / 100) %>% 
  group_by(year) %>% 
  summarize(mean_proportion_excellent = mean(prop_excellent), sd_proportion_excellent = sd(prop_excellent)) %>% 
  knitr::kable()
```
From this table, we can see that the mean decreases from 2002 to 2006, and remains the same to 2010. The standard deviations remain roughly the same. 

##Part D: For each year and state, I compute the average proportion in each response category (taking the average across locations in a state). Then, I make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.
```{R}
brfss_data %>% 
  mutate(prop_excellent = data_value / 100) %>% 
  group_by(year, locationabbr, response) %>% 
  summarize(mean_proportion = mean(prop_excellent)) %>% 
  ggplot(aes(x = year, y = mean_proportion, fill = locationabbr)) +
    geom_boxplot(aes(group = cut_width(year,1))) +
    scale_x_continuous(breaks = c(2003, 2006, 2009)) +
    facet_grid(~response) +
    labs(
      title = "Response to Overall Health Question: Broken Down By State from 2002 to 2010",
      x = "Year", 
      y = "Mean Proportion",
      caption = "BRFSS Data"
    )
```
From these box plots, we can see that over time, the mean proportion of responses remain roughly the same. There is a higher proportion for very good responses and good responses, followed by excellent propotions. There is very few fair responses and even less poor responses. Also, the poor responses have very little variation, with many outliers compared to the other response groups. 

#Question 2
##Data Cleaning of the instacart dataset. 
```{r question 2 cleaning}
library(p8105.datasets)
data("instacart")

instacart = instacart %>% 
janitor::clean_names()
```

```{r inline codes 1}

dim(instacart)

instacart %>% 
  select(user_id) %>% 
  head(1)

instacart %>% 
  select(product_name) %>% 
  head(1)

instacart %>% 
  select(department) %>% 
  head(1)
```


The dimensions of this dataset are (`r {dim(instacart)}`). This means there are 1384617 observations (rows) and 15 variables (columns). Some of the key variables include the user identification number, product name (along with an identification number), the department and aisle the product is located in. There is also:
"order_id"               "product_id"            
"add_to_cart_order"      "reordered"             
"user_id"                "eval_set"              
"order_number"           "order_dow"             
"order_hour_of_day"      "days_since_prior_order"
"product_name"           "aisle_id"              
"department_id"          "aisle"                 
"department"   

An example of an observation would be for the user id `r {instacart %>% select(user_id) %>% head(1)}`, a product they ordered was `r {instacart %>% select(product_name) %>% head(1)}`. This product comes from the `r {instacart %>% select(department) %>% head(1)}` department. 

##Part A
```{r q 2 aisles}
instacart %>% 
group_by(aisle) %>% 
  count(aisle_id) %>% 
arrange(desc(n)) %>% 
  head(2) %>% 
  knitr::kable()
```

```{r inline codes 2}
instacart %>% 
  distinct(aisle) %>% 
count()
```

There are `r {instacart %>% 
  distinct(aisle) %>% 
count()}` aisles.
The aisles with the most orders the fresh vegtables and fresh fruits aisles, each with over 150,000 orders. 

##Part B: Here, I make a plot that shows the number of items ordered in each aisle.
```{r question 2 plots}
instacart %>% 
  group_by(aisle) %>% 
  summarize(ordered_amount = n()) %>% 
  ggplot(aes(x = aisle, y = ordered_amount)) + 
  geom_point() +
  labs(
     title = "Items ordered in each aisle",
     x = "aisle",
     y = "number of items ordered"
    ) +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, size = 5))
```
From this plot, we see the number of items ordered from each aisle. There are 2 major outliers in the 150,000 count, which matches our information from the table above, as this is the fresh vegetables and fresh fruits aisles.

##Part C: I make a table showing the most popular item in each of the aisles: “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.
```{R q2: table 1}
instacart %>% 
  filter(aisle == "baking ingredients" | aisle == 
           "dog food care" | aisle == "packaged vegetables fruits") %>% 
  group_by(product_name, aisle) %>% 
  summarize(most_ordered = n()) %>% 
arrange(desc(most_ordered)) %>% 
group_by(aisle) %>% 
  top_n( n = 1, wt = most_ordered) %>% 
  select(aisle, product_name, most_ordered) %>% 
  rename(top_product_name = product_name) %>%
  rename(number_of_orders = most_ordered) %>% 
  arrange(aisle) %>% 
  knitr::kable()
```
In the baking ingredients aisle, light brown sugar was the most ordered item, and was ordered 499 times. IN the dog food care aisle, the snack sticks chicken and rice receipe dog treats were the most orderd, as they were ordered 30 times. Lastly, in the packaged vegetables fruits aisle, the organic baby spinach was the most ordered item, and was ordered 9784 times. 


##Part D: I make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.
```{r q2: table 2}
instacart %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  select(product_name, order_dow, order_hour_of_day) %>% 
  group_by(order_dow, product_name) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_hour) %>% 
  knitr::kable()
```
The times at which Pink Lady Apples and Coffee Ice Cream are ordered can be compared in this table. Pink Lady Apples are ordered earlier in the day than Coffee Ice Cream typically.

#Question 3
##I load the data from NY_NOAA
```{r question 3 data import}
library(p8105.datasets)
data("ny_noaa") 
```

```{r inline codes 3}
dim(ny_noaa)

ny_noaa %>% 
  select(id) %>% 
  head(1)

ny_noaa %>% 
  filter(is.na(tmax) | is.na(tmin) | is.na(prcp) | is.na(snow) | is.na(snwd)) %>% 
  nrow()
```
The dimensions of this dataset are (`r {dim(ny_noaa)}`). This means there are 2595176 observations (rows) and 7 variables (columns).  The information in this table is information about precipitation, temperature, and snow in New York, obtained from weather stations. Each observation is a specific weather station, on a specific date. The variables included are: "id"   "date" "prcp" "snow" "snwd" "tmax" "tmin". The id is a idnetificatoin number given to each of the weather stations (there are 5 in total). For example, `r {ny_noaa %>% select(id) %>% head(1)}` is an ID. Then there is thee date the observation is referring to, it has the day, month and year. The prcp variable is the amount of precipitation in tenths of a millimeter, while tmax and tmin are the maximum and minimum temperatures in tenths of a degree (Celsius). There is also measurements of snow, which it is important to note does not occur year-round. 

Missing values is an issue within this dataset. There are  `r {ny_noaa %>% filter(is.na(tmax) | is.na(tmin) | is.na(prcp) | is.na(snow) | is.na(snwd)) %>% nrow()}` observations where at least one piece of weather information is missing. Thus, around 50% of observations in this dataset have missing weather information.

##I clean the data from NY_NOAA and create separate variables for year, month, and day.
```{r question 3 data cleaning}
c_ny_noaa = ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(prcp = prcp / 10, tmax = as.integer(tmax) / 10, tmin = as.integer(tmin) / 10)

c_ny_noaa %>% 
  group_by(snow) %>% 
  summarize(freq = n()) %>% 
  arrange(desc(freq)) %>% 
  head(3)
```
For snowfall, the most common values are actually 0 inches and N/A, which makes sense because snow does not occur year-long. Following this, the most common snowfall is 25 inches. 
##Part A: I make a two-panel plot showing the average max temperature in January and in July in each station across years.
```{r Q3: plot 1}
c_ny_noaa %>% 
  filter(tmax != "NA" & (month == "01" | month == "07")) %>% 
  mutate(month = month.name[as.integer(month)]) %>%
  group_by(id, year, month) %>% 
  summarize(avg_temp = mean(tmax)) %>%
  ggplot(aes(x = year, y = avg_temp, color = id)) +
    geom_point(alpha = 0.3) +
    scale_x_discrete(breaks = c(1980, 1990, 2000, 2010)) +
  facet_grid(~month) +  
  labs(
      x = "Year",
      y = "Average Temperature (°C)",
      title = "Average Maximum Temperatures for January and July in New York",
      caption = "NY_NOAA Data"
    ) +
    viridis::scale_color_viridis(
      discrete = TRUE,
      name = "ID"
    ) +
    theme_bw() +
    theme(legend.position = "bottom", legend.text = element_text(size = 3), legend.key.size = unit(1, "point")) +
    guides(color = guide_legend(nrow = 22))
```
About the plot: We see that the average maximum temperature across all locations is lower in January than it is in July, which is expected. We can also see that there is greater variation in in January than in July across most locations, as the range is wider. We are able to see outliers, though due to the number of locations I cannot distiguish exactly which locations. There appears to be more outliers in the later years for both months. For the most part, it appears that the overall temperature range has stayed he same, with yearly variation across both months. We could say that there is more variation in January than in July. 
##Part B: I Make a two-panel plot showing (i) tmax vs tmin for the full dataset ; and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.
```{R Q3: last plot}
library(hexbin)
temp_plot = c_ny_noaa %>% 
  filter(tmax != "NA" & tmin != "NA") %>% 
  ggplot(aes(x = tmax, y = tmin)) + 
    stat_binhex() + 
  labs(
      x = "maximum temperature (°C)",
      y = "minimum temperature (°C)",
      title = "Maximum and minimum temperatures",
      caption = "Data from NOAA"
    )

snowfall_plot = c_ny_noaa %>% 
  filter(snow != "NA" & snow > 0 & snow < 100) %>% 
  ggplot(aes(x = year, y = snow)) +
  geom_boxplot() +
    labs(
      x = "year",
      y = "snowfall amount (mm)",
      title = "Yearly distribution of snowfall",
      caption = "NOAA data"
    )

ggpubr::ggarrange(temp_plot, snowfall_plot,
  ncol = 1, nrow = 2)
```
The first plot shows a comparison of the maximum and minimum temperatures. There is a trend in that as the maximum temperature increases, the minimum temperature also tends to increase. The second plot shows the yearly distribution of snowfall has stayed constant, with very few outliers. 